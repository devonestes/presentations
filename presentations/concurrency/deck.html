---
layout: presentation
title: Comparing Concurrency Patterns in Elixir and Erlang
permalink: /elixir_erlang_concurrency/
---
layout: true
.watermark-left[@devoncestes]
.watermark-middle[CBL Berlin 2019]

---

class: center, middle, headline

## Comparing Common Concurrency Patterns
### in Elixir and Erlang

???
My name is Devon...

One of the things that many people love about the BEAM is the wonderful support
for concurrent computation. Not just concurrent, but truly parallel. If you want
to peg all of your CPU cores at 100%, you can easily do that thanks to the BEAM.

But the thing is, you still need to design those systems. And that's what we're
going to be talking about today. Specifically, we're going to be looking at two
open source examples of some concurrent computation.

---
class: center, middle, headline

## Embarrassingly Parallel

---
class: center, middle

![Recursion](/assets/images/recursion.gif)

???

This basically means that we have a problem that can be broken down into smaller
sub-problems. And occasionally those sub-problems can also be broken down
further as well. And all of these divisions can be run in parallel. You may of
heard of something called map-reduce before spoken about in this sense - well,
that's sort of what this is.

---

class: center, middle

# Benchee vs. ExUnit
# &nbsp;

---
class: center, middle

# ~~Benchee vs. ExUnit~~
# Parallel Map vs. Event Manager

???

Both ExUnit and Benchee have some parallel computation involved in their
execution. Specifically, both of these problems are - and I'm not making this
term up here - embarassingly parallel. This term is for problems that can be
executed entirely in parallel without any communication between the processes.

Benchee and ExUnit handle this basic problem in two ways. We're going to call
those two options Parallel Map Reduce and an Event Manager. Each of these
implementations has some pros and cons - that's like the story of all software,
right? It's all about tradeoffs.

So, let's look at these two implemenations, discuss the tradeoffs, and hopefully
we'll have a deeper understanding of these two options so you can make some good
choices when it comes time to implement something like this yourselves.

The simpler of the two is the parallel map reduce that we do in Benchee. Looking
at the actual code, it's pretty simple.

---
class: padding

.center[
# Parallel Map
]

```
@spec map(Enum.t(), fun) :: list
def map(collection, func) do
  collection
  |> Enum.map(&Task.async(fn -> func.(&1) end))
  |> Enum.map(&Task.await(&1, :infinity))
end
```

???

We have a collection here that we can map over, and we have a function that is
going to be applied for each element in the collection. We're using the
wonderful Task module for this to run these operations in parallel, and then
waiting for all of them to finish.

But I want to go one level deeper here, because I think it's kind of important
to know what's actually going on in those Task functions.

---
class: padding

.center[
# Parallel Map
]

```
@spec map(Enum.t(), fun) :: list
def map(collection, func) do
  collection
  |> Enum.map(&`Task.async`(fn -> func.(&1) end))
  |> Enum.map(&`Task.await`(&1, :infinity))
end
```

???

We have a collection here that we can map over, and we have a function that is
going to be applied for each element in the collection. We're using the
wonderful Task module for this to run these operations in parallel, and then
waiting for all of them to finish.

But I want to go one level deeper here, because I think it's kind of important
to know what's actually going on in those Task functions.



---

class: padding

.center[
# Basic async & await
]

```
def async(func) do
  ref = make_ref()
  me = self()
  spawn(fn -> send(me, {ref, func()}) end)
  ref
end
```

--

```
def await(ref, timeout \\ :infinity) do
  receive do
    {^ref, value} -> value
    after timeout -> {:error, :timeout}
  end
end
```

???

This is an extremely simplified version of what's going on in those functions,
but the important parts are still there. The most important thing to see here is
that while we _are_ doing all of the computation in parallel, we're blocking
this process with those receive blocks until all of the functions that we're
processing in parallel finish.

---
class: padding

.hidden[
# _
]

```
me = self()

fun = fn seconds ->
  Process.sleep(seconds * 1000)    # Simulate some expensive work
  send(me, Time.utc_now().second)
end
```

--

```
Time.utc_now().second              #=> 48
```

--

```
Parallel.map([1, 2, 3, 1], fun)    #=> [49, 50, 51, 49]
```

--

```
Time.utc_now().second              #=> 51
```

???

So, if we do something like this, we will get one message in one second, and
then our process won't receive the third and fourth messages until after that
second message is received. It'll be in the mailbox for sure, but it won't be
received by the process because we're receiving things in specific order there.

---
class: padding

.center[
# Distributed Async & Await
]

```
def async(func, `node \\ Node.self()`) do
  ref = make_ref()
  me = self()
  `Node.spawn(node`, fn -> send(me, {ref, func()}) end)
  ref
end

def await(ref, timeout) do
  receive do
    {^ref, value} -> value
    after timeout -> {:error, :timeout}
  end
end
```

???

And this is only tangentially related, but I always like showing it because it's
just so damn cool. Before what we had was an implementation that was using 100%
of the CPU cycles on our node. But now, what we have here with literally one
line changed, and only barely so, is a distributed implementation that will use
100% of the CPU cycles on every node in our cluster. Going from a single machine
to a distributed map-reduce is just that easy!

This is also where I have to put the disclaimer that handling all the myriad
failure possiblities with distributed systems is extremely complicated, and that
this won't reliably work in production. But still, it's cool, right?!



---
class: center, middle

# `Task.yield`
# `Task.async_stream`

???

So this is one important thing to consider here. When we're mapping over the
elements in that list, we're blocking on each element in order. This is nice and
simple, but it does have some drawbacks. The Task module is rather simple and
really cool, and it's a super example of OTP if that's something that you're
still a little confused by. There's supervision and all that stuff going on
there, so I'd really encourage y'all to check itout.

But this stuff, as cool as it is, wouldn't work for ExUnit. Let's look at why.

---
class: padding

.center[
# Event Manager
]

```
def run() do
  # ...
end

def loop(:async) do
  # ...
end

def loop(:sync) do
  # ...
end

def run_module(mod) do
  # ...
end

def run_test(test) do
  # ...
end
```

???

Ok, so this is an extremely simple version of how ExUnit runs your tests. But,
you'll see here that we're running some modules in parallel, and others in
sequence. We have that EventManager that seems to be really important. What we
have here is a lot of flexibility, which is awesome. It's also far more
complicated than what we had before. Could we simplify this?

---
class: padding

.hidden[
# _
]

```
def run() do
  EventManager.start()

  EventManager.suite_started()
  loop(:async)
  EventManager.suite_finished()

  results = EventManager.get_results()
  EventManger.stop()
  results
end
```

---
class: padding

.hidden[
# _
]

```
def loop(:async) do
  case wait_for_module(:async) do
    {:ok, mod} ->
      spawn(fn -> run_module(mod) end)
      loop(:async)

    {:error, :no_more_async_modules} ->
      loop(:sync)
  end
end
```
---
class: padding

.hidden[
# _
]

```
def loop(:async) do
  case `wait_for_module(:async)` do
    {:ok, mod} ->
      spawn(fn -> run_module(mod) end)
      loop(:async)

    {:error, :no_more_async_modules} ->
      loop(:sync)
  end
end
```

---
class: padding

.hidden[
# _
]

```
def loop(:async) do
  case wait_for_module(:async) do
    `{:ok, mod}` ->
      spawn(fn -> run_module(mod) end)
      loop(:async)

    {:error, :no_more_async_modules} ->
      loop(:sync)
  end
end
```

---
class: padding

.hidden[
# _
]

```
def loop(:async) do
  case wait_for_module(:async) do
    {:ok, mod} ->
      `spawn(fn -> run_module(mod) end)`   # non-blocking
      loop(:async)

    {:error, :no_more_async_modules} ->
      loop(:sync)
  end
end
```

---
class: padding

.hidden[
# _
]

```
def loop(:async) do
  case wait_for_module(:async) do
    {:ok, mod} ->
      spawn(fn -> run_module(mod) end)   # non-blocking
      `loop(:async)`

    {:error, :no_more_async_modules} ->
      loop(:sync)
  end
end
```

---
class: padding

.hidden[
# _
]

```
def loop(:async) do
  case wait_for_module(:async) do
    {:ok, mod} ->
      spawn(fn -> run_module(mod) end)   # non-blocking
      loop(:async)

    `{:error, :no_more_async_modules}` ->
      loop(:sync)
  end
end
```

---
class: padding

.hidden[
# _
]

```
def loop(:async) do
  case wait_for_module(:async) do
    {:ok, mod} ->
      spawn(fn -> run_module(mod) end)   # non-blocking
      loop(:async)

    {:error, :no_more_async_modules} ->
      `loop(:sync)`
  end
end
```

---
class: padding

.hidden[
# _
]

```
def loop(:async) do
  case wait_for_module(:async) do
    {:ok, mod} ->
      spawn(fn -> run_module(mod) end)   # non-blocking
      loop(:async)

    {:error, :no_more_async_modules} ->
      loop(:sync)
  end
end

def loop(:sync) do
  case wait_for_module(:sync) do
    {:ok, mod} ->
      run_module(mod)
      loop(:sync)

    {:error, :no_more_modules} ->
      :ok
  end
end
```

---
class: padding

.hidden[
# _
]

```
def loop(:async) do
  case wait_for_module(:async) do
    {:ok, mod} ->
      spawn(fn -> run_module(mod) end)   # non-blocking
      loop(:async)

    {:error, :no_more_async_modules} ->
      loop(:sync)
  end
end

def loop(:sync) do
  case `wait_for_module(:sync)` do
    {:ok, mod} ->
      run_module(mod)
      loop(:sync)

    {:error, :no_more_modules} ->
      :ok
  end
end
```

---
class: padding

.hidden[
# _
]

```
def loop(:async) do
  case wait_for_module(:async) do
    {:ok, mod} ->
      spawn(fn -> run_module(mod) end)   # non-blocking
      loop(:async)

    {:error, :no_more_async_modules} ->
      loop(:sync)
  end
end

def loop(:sync) do
  case wait_for_module(:sync) do
    `{:ok, mod}` ->
      run_module(mod)
      loop(:sync)

    {:error, :no_more_modules} ->
      :ok
  end
end
```

---
class: padding

.hidden[
# _
]

```
def loop(:async) do
  case wait_for_module(:async) do
    {:ok, mod} ->
      spawn(fn -> run_module(mod) end)   # non-blocking
      loop(:async)

    {:error, :no_more_async_modules} ->
      loop(:sync)
  end
end

def loop(:sync) do
  case wait_for_module(:sync) do
    {:ok, mod} ->
      `run_module(mod)`                   # blocking
      loop(:sync)

    {:error, :no_more_modules} ->
      :ok
  end
end
```

---
class: padding

.hidden[
# _
]

```
def loop(:async) do
  case wait_for_module(:async) do
    {:ok, mod} ->
      spawn(fn -> run_module(mod) end)   # non-blocking
      loop(:async)

    {:error, :no_more_async_modules} ->
      loop(:sync)
  end
end

def loop(:sync) do
  case wait_for_module(:sync) do
    {:ok, mod} ->
      run_module(mod)                   # blocking
      `loop(:sync)`

    {:error, :no_more_modules} ->
      :ok
  end
end
```

---
class: padding

.hidden[
# _
]

```
def loop(:async) do
  case wait_for_module(:async) do
    {:ok, mod} ->
      spawn(fn -> run_module(mod) end)   # non-blocking
      loop(:async)

    {:error, :no_more_async_modules} ->
      loop(:sync)
  end
end

def loop(:sync) do
  case wait_for_module(:sync) do
    {:ok, mod} ->
      run_module(mod)                   # blocking
      loop(:sync)

    `{:error, :no_more_modules}` ->
      :ok
  end
end
```

---
class: padding

.hidden[
# _
]

```
def loop(:async) do
  case wait_for_module(:async) do
    {:ok, mod} ->
      spawn(fn -> run_module(mod) end)   # non-blocking
      loop(:async)

    {:error, :no_more_async_modules} ->
      loop(:sync)
  end
end

def loop(:sync) do
  case wait_for_module(:sync) do
    {:ok, mod} ->
      run_module(mod)                   # blocking
      loop(:sync)

    {:error, :no_more_modules} ->
      `:ok`
  end
end
```

---
class: padding

.hidden[
# _
]

```
def run_module(mod) do
  EventManager.module_started(mod)

  mod
  |> get_tests()
  |> Enum.map(&run_test/1)

  EventManager.module_finished(mod)
end
```
--
```
def run_test(test) do
  EventManager.test_started(test)
  result = run(test)
  EventManager.test_finished(result)
end
```

???

Ok, so this is an extremely simple version of how ExUnit runs your tests. But,
you'll see here that we're running some modules in parallel, and others in
sequence. We have that EventManager that seems to be really important. What we
have here is a lot of flexibility, which is awesome. It's also far more
complicated than what we had before. Could we simplify this?

---
class: padding

.center[
# ExUnit As Parallel Map
]

```
def run() do
  load_modules()
  |> Parallel.map(&run_module/1)
  |> format_results()
end

def run_module(mod) do
  mod
  |> get_tests()
  |> Enum.map(&run_test/1)
end
```

???

This is much simpler! It will load all of our modules at once, and then it will
run all of those modules in parallel. This is super easy, but it doesn't
actually do what ExUnit needs, and that's why they use that EventManager
pattern.

The EventManager offers flexiblity to do really cool stuff, including mixing
parallel and serial computation. It's basically a GenServer that's holding all
the state for the process and listening to multiple possible messages. When we
do parallel map that's such a simple and kind of dumb solution, but it's also
really a great option for most cases where you need parallel computation.

But the EventManager handles that complexity quite well by providing a neat
interface and hiding all of the complexity from its caller. Looking at my
simplified version of ExUnit's runner there, we can get a pretty good idea of
what's going on in that module without having to look at the implementation.

I also really like that the EventManager here can sort of be seen as a fancy
state machine! I guess it's like some sort of state machine that's made up of
other state machines (because recursion is everywhere in nature), but that - to
me at least - makes a lot of sense conceptually and I think aides both in
understanding this problem and in maintaining this code. As someone who has done
a fair amount of work on this part of ExUnit over the years, it was super simple
for me to dive in and make those changes, so kudos to the folks that designed
that originally and expanded on it over the years.

---
class: middle, center

# State Machines are great!

### `:gen_statem`
### `:gen_event`

---

.center[
# The tradeoffs
]

.cols[
.fifty[
.center[
# Parallel Map
]

- Simple!
- Rigid
- No coordination between units
]

.fifty[
.center[
# Event Manager
]

- Complex (but still not so bad)
- Totally flexible
- Single point of coordination between units (great for side effects)
]
]

???

And so, like most things, we come down to tradeoffs. And like most tradeoffs,
we're trading simplicity for flexibility. The simplest thing is indeed still
quite good, and can even be improved rather easily if you like.

I would strongly encourage you all to start with something really simple like a
parallel map when you're adding concurrency to your applictions. 99 times out of
100 that's probably going to get you pretty far.

But, the minute that simple pattern stops working for you, I strongly encourage
you to create some sort of managed interface for your concurent operation. Try
thinking about your operation as a state machine. Ask yourself questions like
"Where do I start? What states can this process be in? What are the triggers
for moving between states?" I find that if you ask yourself these kinds of
questions, a really elegant and natural interface for your manager just sort of
falls into place.

State machines are really nifty, ya know?!
