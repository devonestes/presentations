---
layout: presentation
title: Comparing Concurrency Patterns in Elixir and Erlang
permalink: /elixir_erlang_concurrency/
---
layout: true
.watermark-left[@devoncestes]
.watermark-middle[LambdaDays 2020]

---
class: center, middle, headline

## Comparing Common Concurrency Patterns
### in Elixir and Erlang

???
My name is Devon...

One of the things that many people love about the BEAM is the wonderful support
for concurrent computation. Not just concurrent, but truly parallel. If you want
to peg all of your CPU cores at 100%, you can easily do that thanks to the BEAM.

But the thing is, you still need to design those systems. And that's what we're
going to be talking about today. Specifically, we're going to be looking at two
open source examples of some concurrent computation.

---
class: center, middle

![](/assets/images/sketch_logo.png)

---
class: center, middle

![](/assets/images/pattern_language.png)

???
A while back a preliminary paper was published on this topic, and that's what
we're going to use as the basis for today's talk. There are other people who
have come up with similar terms for these patterns, some of which differ, but
today we'll be using these terms. I think they're good ones.

---
class: center, middle

![](/assets/images/PDSE-algorithm-structure.gif)

???
For me it all starts at this awesome little flowchart. And before we get into
this too deeply, let's just cut down our scope a little bit.

---
class: center, middle

![](/assets/images/photo_2020-01-12_15-11-03.jpg)

---
class: center, middle

![](/assets/images/photo_2020-01-12_15-11-06.jpg)

---
class: center, middle

![](/assets/images/photo_2020-01-12_15-11-09.jpg)

---
class: center, middle

![](/assets/images/photo_2020-01-12_15-11-12.jpg)

---
class: center, middle

![](/assets/images/photo_2020-01-12_15-11-16.jpg)

---
class: center, middle

![](/assets/images/photo_2020-01-12_15-11-19.jpg)

---
class: center, middle

![](/assets/images/photo_2020-01-12_15-11-22.jpg)

---
class: center, middle

![](/assets/images/photo_2020-01-12_15-11-27.jpg)

---
class: center, middle

![](/assets/images/photo_2020-01-12_15-11-30.jpg)

---
class: middle, padding

.center[
]

.cols[
.fifty[
.center[
]

# Benchee
# &nbsp;<br>&nbsp;
# &nbsp;
]

.fifty[
.center[
]

# Reduction
# &nbsp;<br>&nbsp;
# &nbsp;
]
]

---
class: middle, padding

.center[
]

.cols[
.fifty[
.center[
]

# Benchee
# ExUnit<br>&nbsp;
# &nbsp;
]

.fifty[
.center[
]

# Reduction
# Asynchronous Decomposition
# &nbsp;
]
]

---
class: middle, padding

.center[
]

.cols[
.fifty[
.center[
]

# Benchee
# ExUnit<br>&nbsp;
# GenStage
]

.fifty[
.center[
]

# Reduction
# Asynchronous Decomposition
# Pipeline Processing
]
]

???

---
class: center, middle, headline

## Embarrassingly Parallel

???
So there's that fourth one, which has my favorite name out of the bunch by far,
but the thing is, this one is so simple I didn't think it was worth covring. If
your language has actors, then this is as simple as just spawning a process and
letting it do its thing. That's it.

For a canonical exaple, imagine that you have a list of 1,000 email addresses, and
you want to send an email to all of those people concurrently. Take the literally 8
seconds needed to come up with a solution to that problem in your language.

That's it. We can move on now, right?
---
class: padding

.center[
# Embarrassingly Parallel
]

```
def send_emails(emails) do # serial
  Enum.map(emails, fn email ->
    send_email(email)
  end)
end
```

---
class: padding

.center[
# Embarrassingly Parallel
]

```
def send_emails(emails) do # serial
  Enum.map(emails, fn email ->
    send_email(email)
  end)
end

def send_emails(emails) do # parallel
  Enum.map(emails, fn email ->
    spawn(fn -> send_email(email) end)
  end)
end
```

---
class: padding

.center[
# Embarrassingly Parallel
]

```
def send_emails(emails) do # serial
  Enum.map(emails, fn email ->
    send_email(email)
  end)
end

def send_emails(emails) do # parallel
  Enum.map(emails, fn email ->
    `spawn`(fn -> send_email(email) end)
  end)
end
```

---
class: center, middle

![](/assets/images/photo_2020-01-12_21-59-12.jpg)

???
1

---
class: center, middle

![](/assets/images/photo_2020-01-12_21-59-08.jpg)

???
2

---
class: center, middle

![](/assets/images/photo_2020-01-12_21-58-46.jpg)

???
3

---
class: center, middle

![](/assets/images/photo_2020-01-12_21-58-54.jpg)

???
6
---
class: center, middle

![](/assets/images/photo_2020-01-12_21-58-58.jpg)

???
7

---
class: center, middle

![](/assets/images/photo_2020-01-12_21-58-44.jpg)

???
5
---
class: center, middle

![](/assets/images/photo_2020-01-12_21-58-41.jpg)

???
12
---
class: center, middle

![](/assets/images/photo_2020-01-12_21-58-35.jpg)

???
10
---

class: padding

.center[
# Embarrassingly Parallel
]

- ## Tasks are independent

--

- ## No communication

--

- ## No coordination

--

- ## Just set it - and forget it!

---
class: center, middle, headline

## Reduction

???
Super common, probably more common than embarrasingly parallel.

Think of parallel map, or async/await. It's like embarrasingly
parallel, but where you actually care about the result.

---
class: padding

.center[
# Reduction
]

```
defmodule Parallel do
  def map(collection, func) do
    refs =
      Enum.map(collection, fn element ->
        Task.async(fn -> func.(element) end)
      end)

    Enum.map(refs, fn ref ->
      Task.await(ref, :infinity))
    end)
  end
end
```

???

We have a collection here that we can map over, and we have a function that is
going to be applied for each element in the collection. We're using the
wonderful Task module for this to run these operations in parallel, and then
waiting for all of them to finish.

But I want to go one level deeper here, because I think it's kind of important
to know what's actually going on in those Task functions.

---
class: padding

.center[
# Reduction
]

```
defmodule Parallel do
  def map(collection, func) do
    refs =
      Enum.map(collection, fn element ->
        `Task.async`(fn -> func.(element) end)
      end)

    Enum.map(refs, fn ref ->
      `Task.await`(ref, :infinity))
    end)
  end
end

```

???

We have a collection here that we can map over, and we have a function that is
going to be applied for each element in the collection. We're using the
wonderful Task module for this to run these operations in parallel, and then
waiting for all of them to finish.

But I want to go one level deeper here, because I think it's kind of important
to know what's actually going on in those Task functions.

---

class: padding

.center[
# Basic async & await
]

```
def async(func) do
  ref = make_ref()
  me = self()
  spawn(fn -> send(me, {ref, func()}) end)
  ref
end
```

---

class: padding

.center[
# Basic async & await
]

```
def async(func) do
  ref = `make_ref()`
  me = self()
  spawn(fn -> send(me, {ref, func()}) end)
  ref
end
```

---

class: padding

.center[
# Basic async & await
]

```
def async(func) do
  ref = make_ref()
  `me` = self()
  spawn(fn -> send(me, {ref, func()}) end)
  ref
end
```

---

class: padding

.center[
# Basic async & await
]

```
def async(func) do
  ref = make_ref()
  me = self()
  `spawn(fn -> send(me, {ref, func()}) end)`
  ref
end
```

---

class: padding

.center[
# Basic async & await
]

```
def async(func) do
  ref = make_ref()
  me = self()
  spawn(fn -> send(me, {ref, func()}) end)
  `ref`
end
```

---

class: padding

.center[
# Basic async & await
]

```
def async(func) do
  ref = make_ref()
  me = self()
  spawn(fn -> send(me, {ref, func()}) end)
  ref
end
```

```
def await(ref, timeout \\ :infinity) do
  receive do
    {^ref, value} -> value
    after timeout -> {:error, :timeout}
  end
end
```

---

class: padding

.center[
# Basic async & await
]

```
def async(func) do
  ref = make_ref()
  me = self()
  spawn(fn -> send(me, {ref, func()}) end)
  ref
end
```

```
def await(ref, timeout \\ :infinity) do
  `receive` do
    {^ref, value} -> value
    after timeout -> {:error, :timeout}
  end
end
```

---

class: padding

.center[
# Basic async & await
]

```
def async(func) do
  ref = make_ref()
  me = self()
  spawn(fn -> send(me, {ref, func()}) end)
  ref
end
```

```
def await(ref, timeout \\ :infinity) do
  receive do
    {`^ref`, value} -> value
    after timeout -> {:error, :timeout}
  end
end
```

---

class: padding

.center[
# Basic async & await
]

```
def async(func) do
  ref = make_ref()
  me = self()
  spawn(fn -> send(me, {ref, func()}) end)
  ref
end
```

```
def await(ref, timeout \\ :infinity) do
  receive do
    {^ref, value} -> `value`
    after timeout -> {:error, :timeout}
  end
end
```

---

class: padding

.center[
# Basic async & await
]

```
def async(func) do
  ref = make_ref()
  me = self()
  spawn(fn -> send(me, {ref, func()}) end)
  ref
end
```

```
def await(ref, timeout \\ :infinity) do
  receive do
    {^ref, value} -> value
    `after timeout` -> {:error, :timeout}
  end
end
```

???

This is an extremely simplified version of what's going on in those functions,
but the important parts are still there. The most important thing to see here is
that while we _are_ doing all of the computation in parallel, we're blocking
this process with those receive blocks until all of the functions that we're
processing in parallel finish.

---
class: padding

.hidden[
# _
]

```
me = self()

fun = fn seconds ->
  Process.sleep(seconds * 1000)    # Simulate some expensive work
  send(me, Time.utc_now().second)
end
```

--

```
Time.utc_now().second              #=> 48
```

--

```
Parallel.map([1, 2, 3, 1], fun)    #=> [49, 50, 51, 49]
```

--

```
Time.utc_now().second              #=> 51
```

???

So, if we do something like this, we will get one message in one second, and
then our process won't receive the third and fourth messages until after that
second message is received. It'll be in the mailbox for sure, but it won't be
received by the process because we're receiving things in specific order there.

---
class: center, middle

![](/assets/images/IMG_0210_edit.PNG)

---
class: center, middle

![](/assets/images/IMG_0211_edit.PNG)

---
class: center, middle

![](/assets/images/IMG_0212_edit.PNG)

---
class: center, middle

![](/assets/images/IMG_0213_edit.PNG)

---
class: center, middle

![](/assets/images/IMG_0214_edit.PNG)

---
class: center, middle

![](/assets/images/IMG_0219_edit.PNG)

---
class: center, middle

![](/assets/images/IMG_0220_edit.PNG)

---
class: center, middle

![](/assets/images/IMG_0221_edit.PNG)

---

class: padding

.center[
# Basic async & await
]

```
def async(func) do
  ref = make_ref()
  me = self()
  spawn(fn -> send(me, {ref, func()}) end)
  ref
end
```

```
def await(ref, timeout \\ :infinity) do
  receive do
    {^ref, value} -> value
    after timeout -> {:error, :timeout}
  end
end
```


---
class: padding

.center[
# Distributed Async & Await
]

```
def async(func, `node \\ Node.self()`) do
  ref = make_ref()
  me = self()
  `Node.spawn(node`, fn -> send(me, {ref, func()}) end)
  ref
end

def await(ref, timeout) do
  receive do
    {^ref, value} -> value
    after timeout -> {:error, :timeout}
  end
end
```

???

And this is only tangentially related, but I always like showing it because it's
just so damn cool. Before what we had was an implementation that was using 100%
of the CPU cycles on our node. But now, what we have here with literally one
line changed, and only barely so, is a distributed implementation that will use
100% of the CPU cycles on every node in our cluster. Going from a single machine
to a distributed map-reduce is just that easy!

This is also where I have to put the disclaimer that handling all the myriad
failure possiblities with distributed systems is extremely complicated, and that
this won't reliably work in production. But still, it's cool, right?!

---
class: padding

.center[
# Reduction
]

- ## Ordered, fixed size collections (eager evaluation)

--

- ## No shared dependencies between partitions

--

- ## No communication between partitions

--

- ## Communication/coordination with parent allowed

---
class: center, middle, headline

## Asynchronous Decomposition

---
class: padding

.center[
# Asynchronous Decomposition
]

```
def run() do
  # ...
end

def loop(:async) do
  # ...
end

def loop(:sync) do
  # ...
end

def run_module(mod) do
  # ...
end

def run_test(test) do
  # ...
end
```

???

Ok, so this is an extremely simple version of how ExUnit runs your tests. But,
you'll see here that we're running some modules in parallel, and others in
sequence. We have that EventManager that seems to be really important. What we
have here is a lot of flexibility, which is awesome. It's also far more
complicated than what we had before. Could we simplify this?

---
class: padding

.center[
# Asynchronous Decomposition
]

```
def run() do
  EventManager.start()

  EventManager.suite_started()
  loop(:async)
  EventManager.suite_finished()

  results = EventManager.get_results()
  EventManger.stop()
  results
end
```

---
class: padding

.center[
# Asynchronous Decomposition
]

```
def loop(:async) do
  case wait_for_module(:async) do
    {:ok, mod} ->
      spawn(fn -> run_module(mod) end)
      loop(:async)

    {:error, :no_more_async_modules} ->
      loop(:sync)
  end
end
```
---
class: padding

.center[
# Asynchronous Decomposition
]

```
def loop(:async) do
  case `wait_for_module(:async)` do
    {:ok, mod} ->
      spawn(fn -> run_module(mod) end)
      loop(:async)

    {:error, :no_more_async_modules} ->
      loop(:sync)
  end
end
```

---
class: padding

.center[
# Asynchronous Decomposition
]

```
def loop(:async) do
  case wait_for_module(:async) do
    `{:ok, mod}` ->
      spawn(fn -> run_module(mod) end)
      loop(:async)

    {:error, :no_more_async_modules} ->
      loop(:sync)
  end
end
```

---
class: padding

.center[
# Asynchronous Decomposition
]

```
def loop(:async) do
  case wait_for_module(:async) do
    {:ok, mod} ->
      `spawn(fn -> run_module(mod) end)`   # non-blocking
      loop(:async)

    {:error, :no_more_async_modules} ->
      loop(:sync)
  end
end
```

---
class: padding

.center[
# Asynchronous Decomposition
]

```
def loop(:async) do
  case wait_for_module(:async) do
    {:ok, mod} ->
      spawn(fn -> run_module(mod) end)   # non-blocking
      `loop(:async)`

    {:error, :no_more_async_modules} ->
      loop(:sync)
  end
end
```

---
class: padding

.center[
# Asynchronous Decomposition
]

```
def loop(:async) do
  case wait_for_module(:async) do
    {:ok, mod} ->
      spawn(fn -> run_module(mod) end)   # non-blocking
      loop(:async)

    `{:error, :no_more_async_modules}` ->
      loop(:sync)
  end
end
```

---
class: padding

.center[
# Asynchronous Decomposition
]

```
def loop(:async) do
  case wait_for_module(:async) do
    {:ok, mod} ->
      spawn(fn -> run_module(mod) end)   # non-blocking
      loop(:async)

    {:error, :no_more_async_modules} ->
      `loop(:sync)`
  end
end
```

---
class: padding

.center[
# Asynchronous Decomposition
]

```
def loop(:async) do
  case wait_for_module(:async) do
    {:ok, mod} ->
      spawn(fn -> run_module(mod) end)   # non-blocking
      loop(:async)

    {:error, :no_more_async_modules} ->
      loop(:sync)
  end
end

def loop(:sync) do
  case wait_for_module(:sync) do
    {:ok, mod} ->
      run_module(mod)
      loop(:sync)

    {:error, :no_more_modules} ->
      :ok
  end
end
```

---
class: padding

.center[
# Asynchronous Decomposition
]

```
def loop(:async) do
  case wait_for_module(:async) do
    {:ok, mod} ->
      spawn(fn -> run_module(mod) end)   # non-blocking
      loop(:async)

    {:error, :no_more_async_modules} ->
      loop(:sync)
  end
end

def loop(:sync) do
  case `wait_for_module(:sync)` do
    {:ok, mod} ->
      run_module(mod)
      loop(:sync)

    {:error, :no_more_modules} ->
      :ok
  end
end
```

---
class: padding

.center[
# Asynchronous Decomposition
]

```
def loop(:async) do
  case wait_for_module(:async) do
    {:ok, mod} ->
      spawn(fn -> run_module(mod) end)   # non-blocking
      loop(:async)

    {:error, :no_more_async_modules} ->
      loop(:sync)
  end
end

def loop(:sync) do
  case wait_for_module(:sync) do
    `{:ok, mod}` ->
      run_module(mod)
      loop(:sync)

    {:error, :no_more_modules} ->
      :ok
  end
end
```

---
class: padding

.center[
# Asynchronous Decomposition
]

```
def loop(:async) do
  case wait_for_module(:async) do
    {:ok, mod} ->
      spawn(fn -> run_module(mod) end)   # non-blocking
      loop(:async)

    {:error, :no_more_async_modules} ->
      loop(:sync)
  end
end

def loop(:sync) do
  case wait_for_module(:sync) do
    {:ok, mod} ->
      `run_module(mod)`                   # blocking
      loop(:sync)

    {:error, :no_more_modules} ->
      :ok
  end
end
```

---
class: padding

.center[
# Asynchronous Decomposition
]

```
def loop(:async) do
  case wait_for_module(:async) do
    {:ok, mod} ->
      spawn(fn -> run_module(mod) end)   # non-blocking
      loop(:async)

    {:error, :no_more_async_modules} ->
      loop(:sync)
  end
end

def loop(:sync) do
  case wait_for_module(:sync) do
    {:ok, mod} ->
      run_module(mod)                   # blocking
      `loop(:sync)`

    {:error, :no_more_modules} ->
      :ok
  end
end
```

---
class: padding

.center[
# Asynchronous Decomposition
]

```
def loop(:async) do
  case wait_for_module(:async) do
    {:ok, mod} ->
      spawn(fn -> run_module(mod) end)   # non-blocking
      loop(:async)

    {:error, :no_more_async_modules} ->
      loop(:sync)
  end
end

def loop(:sync) do
  case wait_for_module(:sync) do
    {:ok, mod} ->
      run_module(mod)                   # blocking
      loop(:sync)

    `{:error, :no_more_modules}` ->
      :ok
  end
end
```

---
class: padding

.center[
# Asynchronous Decomposition
]

```
def loop(:async) do
  case wait_for_module(:async) do
    {:ok, mod} ->
      spawn(fn -> run_module(mod) end)   # non-blocking
      loop(:async)

    {:error, :no_more_async_modules} ->
      loop(:sync)
  end
end

def loop(:sync) do
  case wait_for_module(:sync) do
    {:ok, mod} ->
      run_module(mod)                   # blocking
      loop(:sync)

    {:error, :no_more_modules} ->
      `:ok`
  end
end
```
---
class: padding

.center[
# Asynchronous Decomposition
]

```
def run_module(mod) do
  EventManager.module_started(mod)

  mod
  |> get_tests()
  |> Enum.map(&run_test/1)

  EventManager.module_finished(mod)
end
```
--
```
def run_test(test) do
  EventManager.test_started(test)
  result = run(test)
  EventManager.test_finished(result)
end
```

???

Ok, so this is an extremely simple version of how ExUnit runs your tests. But,
you'll see here that we're running some modules in parallel, and others in
sequence. We have that EventManager that seems to be really important. What we
have here is a lot of flexibility, which is awesome. It's also far more
complicated than what we had before. Could we simplify this?

---
class: center, middle

![](/assets/images/IMG_0223_edit.PNG)

---
class: center, middle

![](/assets/images/IMG_0224_edit.PNG)


---
class: center, middle

![](/assets/images/IMG_0225_edit.PNG)


---
class: center, middle

![](/assets/images/IMG_0226_edit.PNG)

---
class: center, middle

![](/assets/images/IMG_0227_edit.PNG)

---
class: center, middle

![](/assets/images/IMG_0228_edit.PNG)

---
class: center, middle

![](/assets/images/IMG_0229_edit.PNG)

---
class: center, middle

![](/assets/images/IMG_0230_edit.PNG)

-

---
class: padding

.center[
# Asynchronous Decomposition
]

- ## Collection of unknown size (lazy evaluation)

--

- ## No dependencies between partitions

--

- ## No communication between partitions

--

- ## Communication/coordination with parent allowed

---
class: padding

.center[
# ExUnit As Reduction
]

```
def run() do
  load_modules()
  |> Parallel.map(&run_module/1)
  |> format_results()
end
```

???

This is much simpler! It will load all of our modules at once, and then it will
run all of those modules in parallel. This is super easy, but it doesn't
actually do what ExUnit needs, and that's why they use that EventManager
pattern.

The EventManager offers flexiblity to do really cool stuff, including mixing
parallel and serial computation. It's basically a GenServer that's holding all
the state for the process and listening to multiple possible messages. When we
do parallel map that's such a simple and kind of dumb solution, but it's also
really a great option for most cases where you need parallel computation.

But the EventManager handles that complexity quite well by providing a neat
interface and hiding all of the complexity from its caller. Looking at my
simplified version of ExUnit's runner there, we can get a pretty good idea of
what's going on in that module without having to look at the implementation.

I also really like that the EventManager here can sort of be seen as a fancy
state machine! I guess it's like some sort of state machine that's made up of
other state machines (because recursion is everywhere in nature), but that - to
me at least - makes a lot of sense conceptually and I think aides both in
understanding this problem and in maintaining this code. As someone who has done
a fair amount of work on this part of ExUnit over the years, it was super simple
for me to dive in and make those changes, so kudos to the folks that designed
that originally and expanded on it over the years.

---
class: padding, center, headline, middle

## State Machine

???

The hard part of this pattern is that we don't know when we're done because we're loading our
colleciton asynchronously. That means we need some way to keep track of that, and a state machine
is a great way to do that!

---
class: padding, center, headline, middle

## Pipeline Processing

???

This one should sound pretty familiar, and it is! The general idea is simple, but the thing is, if
you do the simple thing, you're pretty quickly going to end up with some headaches. That's why for
this one I'm going to show you a properly good implementation.

THe other reason I'm not going to show code for this one is because, frankly, you shouldn't write
one of these yourself. You should definitely know when to reach for this pattern, but you should
also know that when that time comes to use a well written library.

---
class: center, middle

![](/assets/images/photo_2020-01-12_21-58-28.jpg)

???
1

---
class: center, middle

![](/assets/images/photo_2020-01-12_21-58-25.jpg)

???
2

---
class: center, middle

![](/assets/images/photo_2020-01-12_21-58-23.jpg)

???
3

---
class: center, middle

![](/assets/images/photo_2020-01-12_21-58-14.jpg)

???
4

---
class: center, middle

![](/assets/images/photo_2020-01-12_21-58-11.jpg)

???
5

---
class: center, middle

![](/assets/images/photo_2020-01-12_21-58-08.jpg)

???
6

---
class: center, middle

![](/assets/images/photo_2020-01-12_21-57-34.jpg)

???
7

---
class: center, middle

![](/assets/images/photo_2020-01-12_21-57-37.jpg)

???
8

---
class: center, middle

![](/assets/images/photo_2020-01-12_21-57-41.jpg)

???
9

---
class: center, middle

![](/assets/images/photo_2020-01-12_21-57-44.jpg)

???
10

---
class: center, middle

![](/assets/images/photo_2020-01-12_21-57-47.jpg)

???
11

---
class: center, middle

![](/assets/images/photo_2020-01-12_21-57-49.jpg)

???
12

---
class: center, middle

![](/assets/images/photo_2020-01-12_21-57-54.jpg)

???
13

---
class: center, middle

![](/assets/images/photo_2020-01-12_21-57-57.jpg)

???
14

---
class: padding

.center[
# Pipeline processing
]

- ## Stream (possibly infinite) of data or events

--

- ## Clearly defined communication between stages

--

- ## Dependencies between processes allowed

--

- ## Communication between consumer and parent allowed

---

.center[
.headline[
## The tradeoffs
]
]

.cols[
.fifty[
.center[
.headline[
### Parallel Map
]
]
]

.fifty[
.center[
.headline[
### Event Manager
]
]
]
]

---

.center[
.headline[
## The tradeoffs
]
]

.cols[
.fifty[
.center[
.headline[
### Parallel Map
]
]

- Simple!
]

.fifty[
.center[
.headline[
### Event Manager
]
]

- Complex (but still not so bad)
]
]

---

.center[
.headline[
## The tradeoffs
]
]

.cols[
.fifty[
.center[
.headline[
### Parallel Map
]
]

- Simple!
- Rigid
]

.fifty[
.center[
.headline[
### Event Manager
]
]

- Complex (but still not so bad)
- Totally flexible
]
]


---

.center[
.headline[
## The tradeoffs
]
]

.cols[
.fifty[
.center[
.headline[
### Parallel Map
]
]

- Simple!
- Rigid
- No coordination between units
]

.fifty[
.center[
.headline[
### Event Manager
]
]

- Complex (but still not so bad)
- Totally flexible
- Single point of coordination between units (great for side effects)
]
]

???

And so, like most things, we come down to tradeoffs. And like most tradeoffs,
we're trading simplicity for flexibility. The simplest thing is indeed still
quite good, and can even be improved rather easily if you like.

I would strongly encourage you all to start with something really simple like a
parallel map when you're adding concurrency to your applictions. 99 times out of
100 that's probably going to get you pretty far.

But, the minute that simple pattern stops working for you, I strongly encourage
you to create some sort of managed interface for your concurent operation. Try
thinking about your operation as a state machine. Ask yourself questions like
"Where do I start? What states can this process be in? What are the triggers
for moving between states?" I find that if you ask yourself these kinds of
questions, a really elegant and natural interface for your manager just sort of
falls into place.

State machines are really nifty, ya know?!

---
class: center, middle, headline

# Thank you!
